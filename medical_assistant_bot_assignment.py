# -*- coding: utf-8 -*-
"""Medical Assistant Bot Assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YtbwFBhc69Y4fy_W0ph8UESQ_x335SpE

##Using RAG (Local: Sentence-Transformers + FAISS + FLAN-T5)
"""

!pip -q install pandas numpy sentence-transformers faiss-cpu "transformers>=4.40.0" accelerate

"""##Loading the CSV file from drive"""

import pandas as pd
import numpy as np

csv_path = "/content/drive/MyDrive/coding_challenge/mle_screening_dataset.csv"
df = pd.read_csv(csv_path)
df.head(3)

"""##Convert each document into string"""

def row_to_text(row):
    # compact, readable representation: "col=value; col2=value; ..."
    parts = []
    for c in df.columns:
        v = row[c]
        if pd.isna(v):
            continue
        sv = str(v)
        if len(sv) > 200:
            sv = sv[:200] + "…"
        parts.append(f"{c}={sv}")
    return "; ".join(parts)

docs = df.apply(row_to_text, axis=1).tolist()
len(docs), docs[1]

"""##Embed and build FAISS index"""

import numpy as np
from sentence_transformers import SentenceTransformer
import faiss

model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")  # 384-dim, fast & strong
batch = 128

embs = []
for i in range(0, len(docs), batch):
    embs.append(model.encode(docs[i:i+batch], show_progress_bar=False))
embs = np.vstack(embs).astype("float32")

# Normalize (so inner product ≈ cosine)
faiss.normalize_L2(embs)

index = faiss.IndexFlatIP(embs.shape[1])
index.add(embs)
index.ntotal

"""##Retriever"""

def retrieve(question, k=8):
    q = model.encode([question]).astype("float32")
    faiss.normalize_L2(q)
    scores, idxs = index.search(q, k)  # inner product
    hits = []
    for score, i in zip(scores[0], idxs[0]):
        if i == -1:
            continue
        hits.append({"row_id": int(i), "score": float(score), "text": docs[i]})
    return hits

# Quick test:
retrieve("Who has the highest accuracy?", k=5)

"""#####Local generator (FLAN-T5). Use “small” on CPU; switch to “base” if you have GPU.


"""

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch

GEN_MODEL = "google/flan-t5-small"  # try "google/flan-t5-base" if you have GPU
tok = AutoTokenizer.from_pretrained(GEN_MODEL)
gen = AutoModelForSeq2SeqLM.from_pretrained(GEN_MODEL, torch_dtype=torch.float32, device_map="auto")

"""## RAG QA function (retrieve → synthesize)"""

def rag_answer(question, k=8, max_new_tokens=128):
    hits = retrieve(question, k=k)
    if not hits:
        return "I couldn't find anything relevant in the CSV.", pd.DataFrame()

    # Build context with explicit row ids (citations)
    context_lines = []
    for h in hits:
        context_lines.append(f"[row {h['row_id']}] {h['text']}")
    context = "\n".join(context_lines)

    prompt = (
        "You are an AI assistant specialized in providing information based on the provided documents. "
    "Answer the user's question truthfully and concisely using *only* the following context. "
    "For each statement in your answer, indicate the source row ID(s) like [row 12, row 45]. "
    "If the answer is not present in the context, respond with 'Information not available in the provided documents.'.\n\n"
    f"Question: {question}\n\nContext:\n{context}\n\nAnswer:"
)
    inputs = tok(prompt, return_tensors="pt", truncation=True, max_length=2048).to(gen.device)
    out = gen.generate(**inputs, max_new_tokens=max_new_tokens)
    answer = tok.decode(out[0], skip_special_tokens=True)

    # Return answer + a small view of the retrieved rows for transparency
    import pandas as pd
    row_ids = [h["row_id"] for h in hits]
    view = df.iloc[row_ids].copy()
    view.insert(0, "row_id", row_ids)
    view.insert(1, "score", [h["score"] for h in hits])
    return answer, view

# Example:
ans, ctx = rag_answer("What are the symptoms of Glaucoma?", k=6)
print(ans)
display(ctx.head())



"""##RAGAS based evaluation"""

from datasets import Dataset

# Prepare data for RAGAS evaluation
# We need a dataset with columns: 'question', 'answer', 'contexts', 'ground_truth' (optional)

# Let's create a small sample dataset for evaluation for demonstration purposes
questions = ["What is Glaucoma?", "How to prevent Glaucoma?", "Who is at risk for Glaucoma?"]
ground_truths = [
    "Glaucoma is a group of eye diseases that damage the optic nerve.",
    "Currently, there is no known way to prevent glaucoma.",
    "Anyone can develop glaucoma, but some groups are at higher risk."
    ]

data = {'question': questions, 'ground_truth': ground_truths}
eval_dataset = Dataset.from_dict(data)

# Now, we need to generate answers and retrieve contexts for these questions using our RAG system
answers = []
contexts = []

for question in questions:
    ans, ctx_df = rag_answer(question, k=8) # Using k=8 as in the rag_answer function
    answers.append(ans)
    contexts.append((ctx_df['question'] + " " + ctx_df['answer']).tolist()) # Extract text from the context DataFrame

eval_dataset = eval_dataset.add_column("answer", answers)
eval_dataset = eval_dataset.add_column("contexts", contexts)

print(eval_dataset)

from ragas import evaluate
from ragas.metrics import (
    answer_relevancy,
    faithfulness,
    context_recall,
)

# Define the metrics to use for evaluation
metrics = [
    answer_relevancy,
    faithfulness,
    context_recall,
]

# Run the evaluation
result = evaluate(
    eval_dataset,
    metrics=metrics,
)

# Print the results
print(result)